{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 1 Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLblU_gpfrRQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from thop import profile\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV3P0XNCfuNZ",
        "outputId": "900de73e-3977-4c6f-e028-35bc491181ef"
      },
      "outputs": [],
      "source": [
        "# Check if CUDA is available\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLpt7FTrfvwN",
        "outputId": "f72722ce-bc15-4d03-f688-7000b3bf6bee"
      },
      "outputs": [],
      "source": [
        "# Calculate mean and std of dataset\n",
        "def get_mean_std(dataset, ratio=1):\n",
        "    # Get mean and std by sample ratio\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=int(len(dataset)*ratio), shuffle=True, num_workers=2)\n",
        "\n",
        "    data = next(iter(dataloader))[0]     # get the first iteration data\n",
        "    mean = np.mean(data.numpy(), axis=(0,2,3))\n",
        "    std = np.std(data.numpy(), axis=(0,2,3))\n",
        "    return mean, std\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_mean, train_std = get_mean_std(train_dataset)\n",
        "test_mean, test_std = train_mean, train_std\n",
        "print(train_mean, train_std)\n",
        "print(test_mean, test_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUhAqjSNfxuY"
      },
      "outputs": [],
      "source": [
        "##### data augmentation & normalization #####\n",
        "transform_train = transforms.Compose([\n",
        "    # 同學實作部分：Add data augmentation here\n",
        "    transforms.RandomRotation(15,expand = True),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomVerticalFlip(0.5),\n",
        "    \n",
        "    transforms.ToTensor(), # Transform to tensor\n",
        "    transforms.Normalize(mean=train_mean, std=train_std), # Normalization\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=test_mean, std=test_std),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NltFsUEffz48",
        "outputId": "55e4c378-6e46-4b29-c39f-7025b0a3b73a"
      },
      "outputs": [],
      "source": [
        "# Dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_ds = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "image, label = trainset[0]\n",
        "print(\"image shape: \", image.shape)\n",
        "\n",
        "# Split validation dataset\n",
        "torch.manual_seed(43)     # Ensure reproducibility\n",
        "val_size = 5000       # Take 5000 images as validation set\n",
        "train_size = len(trainset) - val_size\n",
        "train_ds, val_ds = random_split(trainset, [train_size, val_size])\n",
        "print(\"train length: \", len(train_ds))\n",
        "print(\"val length: \", len(val_ds))\n",
        "print(\"test length: \", len(test_ds))\n",
        "\n",
        "# Declare batch size\n",
        "# 學生實作部分：Set batch size\n",
        "BATCH_SIZE = 16\n",
        "trainloader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "validloader = torch.utils.data.DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph1fHTwff2Qq"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, expansion: int = 1, downsample: nn.Module = None,):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # 學生實作部分：Define the two convolutional layers and the shortcut connection\n",
        "        self.expansion = expansion\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels,\n",
        "            out_channels * self.expansion,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 學生實作部分：Define the forward pass using convolutional layers and the shortcut connection\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(ResNet18, self).__init__()\n",
        "        # 學生實作部分：Define the ResNet-18 architecture using BasicBlock\n",
        "        layers = [2, 2, 2, 2]\n",
        "        self.expansion = 1\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=self.in_channels,\n",
        "            kernel_size=7,\n",
        "            stride=2,\n",
        "            padding=3,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(BasicBlock, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(BasicBlock, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(BasicBlock, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(BasicBlock, 512, layers[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * self.expansion, num_classes)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        # 學生實作部分：Define make_layer function to create layers of blocks\n",
        "        downsample = None\n",
        "        if stride != 1:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    self.in_channels,\n",
        "                    out_channels * self.expansion,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels * self.expansion),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(self.in_channels, out_channels, stride, self.expansion, downsample)\n",
        "        )\n",
        "        self.in_channels = out_channels * self.expansion\n",
        "\n",
        "        for i in range(1, num_blocks):\n",
        "            layers.append(\n",
        "                block(self.in_channels, out_channels, expansion=self.expansion)\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 學生實作部分：Define the forward pass of ResNet-18\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        out = self.fc(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6-BvIaDNgGpc",
        "outputId": "c7b9f973-e201-4af5-a061-07f9293dfd5f"
      },
      "outputs": [],
      "source": [
        "# Create SummaryWriter\n",
        "writer = SummaryWriter(\"./tensorboard\")\n",
        "\n",
        "model = ResNet18(num_classes=10)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
        "model = model.to(device)\n",
        "# Print model summary\n",
        "summary(model, (3, 224, 224))\n",
        "\n",
        "# Calculate FLOPs and Params\n",
        "dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "flops, params = profile(model, inputs=(dummy_input, ))\n",
        "print(f\"FLOPs: {flops/1e6:.2f} MFLOPs\")\n",
        "print(f\"Params: {params/1e6:.2f} M\")\n",
        "print(\"device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "collapsed": true,
        "id": "sW_kszMZD5vm",
        "outputId": "219de866-ad15-4b9a-8325-be166fc84ec0"
      },
      "outputs": [],
      "source": [
        "# Setting parameter\n",
        "EPOCH = 10\n",
        "pre_epoch = 5\n",
        "lr = 0.0001\n",
        "\n",
        "# 同學可以根據自己需求調整optimizer, criterion與lr_scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "best_model_path = 'best_resnet18.pth'\n",
        "\n",
        "# Record training and validation metrics\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "best_val_acc = 0.0\n",
        "\n",
        "# Training and validation loop\n",
        "for epoch in range(pre_epoch, EPOCH):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # 學生實作部分：Complete the training and validation loop\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    print('running epoch: {}'.format(epoch))\n",
        "\n",
        "    # train the model\n",
        "    model.train()\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    for data, target in tqdm(trainloader):\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        # update training Accuracy\n",
        "        train_total += target.size(0)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        train_correct += (predicted == target).sum().item()\n",
        "\n",
        "\n",
        "    # validate the model\n",
        "    model.eval()\n",
        "    valid_correct = 0\n",
        "    valid_total = 0\n",
        "    for data, target in tqdm(validloader):\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        target = target.long()\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average validation loss\n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        # update validation Accuracy\n",
        "        valid_total += target.size(0)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        valid_correct += (predicted == target).sum().item()\n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(trainloader.dataset)\n",
        "    valid_loss = valid_loss/len(validloader.dataset)\n",
        "\n",
        "    train_acc = 100 * train_correct / train_total\n",
        "    val_acc = 100 * valid_correct / valid_total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(valid_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "    \n",
        "    \n",
        "    # 儲存最佳模型\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCH}] \"\n",
        "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
        "          f\"Val Loss: {valid_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKJJVv3zGYHI",
        "outputId": "b1b35fdb-26ce-4693-fd32-b10598354590"
      },
      "outputs": [],
      "source": [
        "# Load the best model and evaluate on the test set\n",
        "model.eval()\n",
        "\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "with torch.no_grad():\n",
        "    for test_inputs, test_labels in testloader:\n",
        "        test_inputs, test_labels = test_inputs.to(device), test_labels.to(device)\n",
        "        test_outputs = model(test_inputs)\n",
        "        _, test_predicted = test_outputs.max(1)\n",
        "        test_total += test_labels.size(0)\n",
        "        test_correct += test_predicted.eq(test_labels).sum().item()\n",
        "test_accuracy = 100. * test_correct / test_total\n",
        "print(f\"Best Model Test Accuracy: {test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "SHm67TRfgI_P",
        "outputId": "e8c2762c-0729-441a-e1b3-b6410bf53c08"
      },
      "outputs": [],
      "source": [
        "##### Plot loss & accuracy graph #####\n",
        "import matplotlib.pyplot as plt\n",
        "# 學生實作部分：Plot training and validation loss and accuracy curves\n",
        "def plt_acc_all():\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax1.set_title('All acc')\n",
        "\n",
        "    ax1.plot(train_accuracies)\n",
        "    ax1.plot(val_accuracies)\n",
        "\n",
        "    ax1.legend(['train_acc', 'valid_acc'], loc='upper left')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "def plt_loss_all():\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    ax1 = fig.add_subplot(1, 2, 1)\n",
        "    ax1.set_title('All loss')\n",
        "\n",
        "    ax1.plot(train_losses)\n",
        "    ax1.plot(val_losses)\n",
        "\n",
        "    ax1.legend(['train_loss', 'valid_loss'], loc='upper left')\n",
        "    ax1.set_xlabel('epoch')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plt_loss_all()\n",
        "plt_acc_all()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
