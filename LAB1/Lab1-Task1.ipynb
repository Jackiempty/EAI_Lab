{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b00152",
   "metadata": {},
   "source": [
    "# Lab 1 Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc389ae1",
   "metadata": {},
   "source": [
    "## 1. Implement the following layers as Python functions (both forward and backward propagation)\n",
    "* Inner-product layer\n",
    "* Activation layer(Sigmoid or Rectified)\n",
    "* Softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee38f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, Iterable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce009b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    def __init__(self, data: np.ndarray) -> None:\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "class Module:\n",
    "    def __call__(self, *args, **kwargs) -> np.ndarray:\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        layers = '\\n'.join([f'  ({k}): {v}' for k, v in self.__dict__.items()])\n",
    "        return f'{self.__class__.__name__}(\\n{layers}\\n)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a8912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        init_factor = 0.01\n",
    "        self.W = Parameter(np.random.randn(in_features, out_features) * init_factor)\n",
    "        self.b = Parameter(np.zeros((1, out_features)))\n",
    "\n",
    "        # Cache for backward pass\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 學生實作部分：return output of linear layer\n",
    "        self.x = x\n",
    "        out = x @ self.W.data + self.b.data\n",
    "        return out\n",
    "\n",
    "    def backward(self, dy):\n",
    "        # 學生實作部分：return gradient w.r.t. input and compute gradients for weights and biases\n",
    "        dx = dy @ self.W.data.T\n",
    "        self.W.grad = self.x.T @ dy\n",
    "        self.b.grad = np.sum(dy, axis=0, keepdims=True)\n",
    "        return dx\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(in_features={self.in_features}, out_features={self.out_features})'\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 學生實作部分：return output of ReLU activation\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "\n",
    "    def backward(self, dy):\n",
    "        # 學生實作部分：return gradient w.r.t. input\n",
    "        dx = dy * (self.x > 0)\n",
    "        return dx\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'\n",
    "\n",
    "\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 學生實作部分：return output of Sigmoid activation\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        # 學生實作部分：return gradient w.r.t. input\n",
    "        dx = dy * self.y * (1 - self.y)\n",
    "        return dx\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'\n",
    "\n",
    "\n",
    "class Softmax(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 學生實作部分：return output of Softmax activation\n",
    "        shifted_x = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(shifted_x)\n",
    "        self.y = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return self.y\n",
    "\n",
    "\n",
    "    def backward(self, dy):\n",
    "        # 這邊我們設定同學們會將Softmax和Cross-Entropy Loss一起使用\n",
    "        # 因此backward pass根據講義上的說明使用簡化的版本在Cross-Entropy Loss的部分完成\n",
    "        # 若同學的softmax沒有和Cross-Entropy Loss一起使用，請自行實作完整的backward pass\n",
    "        return dy\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60da9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    def __init__(self, in_features=784, hidden_features=128, num_classes=10) -> None:\n",
    "        # 學生實作部分：design your Model architecture here\n",
    "        # super().__init__()\n",
    "        self.fc1 = Linear(in_features, hidden_features)\n",
    "        self.relu = ReLU()\n",
    "        self.fc2 = Linear(hidden_features, num_classes)\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 學生實作部分：compute forward pass through your model\n",
    "        out = self.fc1.forward(x)\n",
    "        out = self.relu.forward(out)\n",
    "        out = self.fc2.forward(out)\n",
    "        out = self.softmax.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dy):\n",
    "        # 學生實作部分：compute backward pass through your model\n",
    "        dy = self.fc2.backward(dy)\n",
    "        dy = self.relu.backward(dy)\n",
    "        dy = self.fc1.backward(dy)\n",
    "        return dy\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.fc1.parameters() + self.fc2.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896d12f",
   "metadata": {},
   "source": [
    "## 2. Implement training and testing process\n",
    "* included cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd582fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    # root請根據你的檔案位置更改\n",
    "    def __init__(self, root='./data', train=True, transform: Callable = None) -> None:\n",
    "        path = os.path.join(root, 'mnist_train.csv' if train else 'mnist_test.csv')\n",
    "        self.data = np.loadtxt(path, delimiter=',')\n",
    "        self.transform = transform\n",
    "        self.image_size = 28\n",
    "        self.num_classes = 10\n",
    "        self.classes = np.arange(self.num_classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = (self.data[idx, 0] == self.classes).astype(\n",
    "            np.float32\n",
    "        )  # one-hot encoding\n",
    "        image = (\n",
    "            self.data[idx, 1:]\n",
    "            .reshape(self.image_size * self.image_size)\n",
    "            .astype(np.float32)\n",
    "        )\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class Subset:\n",
    "    def __init__(self, dataset, indices: Iterable) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.dataset) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for start_idx in range(0, len(self.dataset), self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, len(self.dataset))\n",
    "            batch_indices = self.indices[start_idx:end_idx]\n",
    "\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "\n",
    "            for idx in batch_indices:\n",
    "                image, label = self.dataset[idx]\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "\n",
    "            yield np.array(batch_images), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f743cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train_imgs, train_labels into training and validation\n",
    "# root請根據你的檔案位置更改\n",
    "def load_mnist_data(\n",
    "    root=\"./data\", batch_size=1, split_ratio=0.1, transform=None\n",
    ") -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    def _split_dataset(dataset, split_ratio):\n",
    "        # 學生實作部分：split dataset into training and validation sets\n",
    "        # hint: return Subset(dataset, train_indices), Subset(dataset, valid_indices)\n",
    "        # 1. total length and index list\n",
    "        dataset_size = len(dataset)\n",
    "        indices = np.arange(dataset_size)\n",
    "\n",
    "        # 2. shuffle\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        # 3. calculate splitting point\n",
    "        split = int(split_ratio * dataset_size)\n",
    "        train_indices = indices[split:]\n",
    "        valid_indices = indices[:split]\n",
    "        return Subset(dataset, train_indices), Subset(dataset, valid_indices)\n",
    "        \n",
    "        \n",
    "\n",
    "    trainset = MNIST(root=root, train=True, transform=transform)\n",
    "    testset = MNIST(root=root, train=False, transform=transform)\n",
    "    trainset, validset = _split_dataset(trainset, split_ratio=split_ratio)\n",
    "    trainldr = DataLoader(trainset, batch_size=batch_size)\n",
    "    validldr = DataLoader(validset, batch_size=batch_size)\n",
    "    testldr = DataLoader(testset, batch_size=batch_size)\n",
    "    return trainldr, validldr, testldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fcfe45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Module):\n",
    "    def __init__(self, epsilon=1e-15) -> None:\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon  # small value to avoid log(0)\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = np.clip(y_pred, self.epsilon, 1 - self.epsilon)\n",
    "        self.y_true = y_true\n",
    "        batch_size = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(self.y_pred)) / batch_size\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        batch_size = self.y_true.shape[0]\n",
    "        grad = (self.y_pred - self.y_true) / batch_size\n",
    "        return grad\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, params: Iterable, lr: float = 1e-3) -> None:\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.data -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "defbfc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model: Module, trainldr: Iterable, criterion, optimizer) -> tuple[float, float]:\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(trainldr):\n",
    "        # 學生實作部分：complete the training process through one epoch\n",
    "        # ========== 1. forward ==========\n",
    "        out = model.forward(x)  # forward pass\n",
    "\n",
    "        # ========== 2. compute loss ==========\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += loss\n",
    "        total += len(y)\n",
    "\n",
    "        # ========== 3. compute accuracy ==========\n",
    "        pred = np.argmax(out, axis=1)\n",
    "        correct += np.sum(pred == y)\n",
    "\n",
    "        # ========== 4. backward ==========\n",
    "        dy = criterion.backward()     # dL/dy_pred\n",
    "        model.backward(dy)\n",
    "\n",
    "        # ========== 5. update ==========\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Hint:\n",
    "        # 1. forward propagation\n",
    "        # 2. compute loss\n",
    "        # 3. compute accuracy\n",
    "        # 4. backward propagation\n",
    "        # 5. update parameters\n",
    "        \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model: Module, testldr: Iterable) -> tuple[float, float]:\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    criterion = CrossEntropyLoss()  # separate criterion from training\n",
    "    for data, target in tqdm(testldr):\n",
    "        # complete the evaluation process\n",
    "        # ========== 1. forward ==========\n",
    "        out = model.forward(data)\n",
    "\n",
    "        # ========== 2. compute loss ==========\n",
    "        loss = criterion(out, target)\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        # ========== 3. compute accuracy ==========\n",
    "        pred = np.argmax(out, axis=1)\n",
    "        correct += np.sum(pred == target)\n",
    "        total += len(target)\n",
    "        # Hint:\n",
    "        # 1. forward propagation\n",
    "        # 2. compute total loss\n",
    "        # 3. compute correct and total\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train(model: MLP, trainldr: Iterable, validldr: Iterable, epochs=10, lr=1e-3):\n",
    "    criterion = CrossEntropyLoss()\n",
    "    # 這邊提供SGD作為optimizer，同學也可以根據自己人需求更換其他optimizer\n",
    "    optimizer = SGD(model.parameters(), lr=lr)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    valid_loss = []\n",
    "    valid_acc = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss, acc = train_one_epoch(model, trainldr, criterion, optimizer)\n",
    "        train_loss.append(loss)\n",
    "        train_acc.append(acc)\n",
    "        print(f'epoch {epoch:d}: train_loss = {loss}, train_acc = {acc}')\n",
    "\n",
    "        loss, acc = evaluate(model, validldr)\n",
    "        valid_loss.append(loss)\n",
    "        valid_acc.append(acc)\n",
    "        print(f'epoch {epoch:d}: valid_loss = {loss}, valid_acc = {acc}\\n')\n",
    "\n",
    "    return train_loss, train_acc, valid_loss, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72aea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: 54000 images\n",
      "validation set: 6000 images\n",
      "test set: 10000 images\n",
      "x shape: (1, 784)\n",
      "y shape: (1, 10)\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=784, out_features=128)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=10)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54000/54000 [00:09<00:00, 5853.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: train_loss = 2.310126360797459, train_acc = 0.7880925925925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:00<00:00, 42476.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: valid_loss = 2.3024319335866723, valid_acc = 0.00016666666666666666\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54000/54000 [00:09<00:00, 5924.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train_loss = 2.302277998554196, train_acc = 0.7985185185185185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:00<00:00, 42397.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: valid_loss = 2.3024341240442183, valid_acc = 0.00016666666666666666\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54000/54000 [00:09<00:00, 5896.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: train_loss = 2.3022780705415085, train_acc = 0.7985185185185185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:00<00:00, 42362.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: valid_loss = 2.3024341240442183, valid_acc = 0.00016666666666666666\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54000/54000 [00:09<00:00, 5893.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: train_loss = 2.3022780705415085, train_acc = 0.7985185185185185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:00<00:00, 42076.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: valid_loss = 2.3024341240442183, valid_acc = 0.00016666666666666666\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54000/54000 [00:09<00:00, 5860.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: train_loss = 2.3022780705415085, train_acc = 0.7985185185185185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:00<00:00, 42391.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: valid_loss = 2.3024341240442183, valid_acc = 0.00016666666666666666\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 18651/54000 [00:03<00:06, 5742.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(net)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# 你可以調整 epochs 和 lr 來觀察不同的訓練效果\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m train_loss, train_acc, valid_loss, valid_acc = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainldr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidldr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.005\u001b[39;49m\n\u001b[32m     22\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, trainldr, validldr, epochs, lr)\u001b[39m\n\u001b[32m     74\u001b[39m valid_acc = []\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     loss, acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainldr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     train_loss.append(loss)\n\u001b[32m     79\u001b[39m     train_acc.append(acc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, trainldr, criterion, optimizer)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# ========== 4. backward ==========\u001b[39;00m\n\u001b[32m     20\u001b[39m dy = criterion.backward()     \u001b[38;5;66;03m# dL/dy_pred\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ========== 5. update ==========\u001b[39;00m\n\u001b[32m     24\u001b[39m optimizer.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mMLP.backward\u001b[39m\u001b[34m(self, dy)\u001b[39m\n\u001b[32m     20\u001b[39m dy = \u001b[38;5;28mself\u001b[39m.fc2.backward(dy)\n\u001b[32m     21\u001b[39m dy = \u001b[38;5;28mself\u001b[39m.relu.backward(dy)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m dy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dy\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mLinear.backward\u001b[39m\u001b[34m(self, dy)\u001b[39m\n\u001b[32m     23\u001b[39m dx = dy @ \u001b[38;5;28mself\u001b[39m.W.data.T\n\u001b[32m     24\u001b[39m \u001b[38;5;28mself\u001b[39m.W.grad = \u001b[38;5;28mself\u001b[39m.x.T @ dy\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28mself\u001b[39m.b.grad = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dx\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Git_repo/AISLab/EAI_Lab/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:2333\u001b[39m, in \u001b[36m_sum_dispatcher\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2327\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPassing `min` or `max` keyword argument when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2328\u001b[39m                          \u001b[33m\"\u001b[39m\u001b[33m`a_min` and `a_max` are provided is forbidden.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[33m'\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m'\u001b[39m, a_min, a_max, out=out, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum_dispatcher\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2334\u001b[39m                     initial=\u001b[38;5;28;01mNone\u001b[39;00m, where=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[32m   2338\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[32m   2339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue,\n\u001b[32m   2340\u001b[39m         initial=np._NoValue, where=np._NoValue):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def transform(x):\n",
    "    \"\"\"map pixels information from range(0, 255) to range(0.01, 1)\"\"\"\n",
    "    return np.asarray(x) * 0.99 + 0.01\n",
    "\n",
    "# \"../data\"請根據你的檔案位置更改\n",
    "trainldr, validldr, testldr = load_mnist_data(\n",
    "    \"./data\", batch_size=1, transform=transform\n",
    ")\n",
    "print(f\"train set: {len(trainldr)} images\")\n",
    "print(f\"validation set: {len(validldr)} images\")\n",
    "print(f\"test set: {len(testldr)} images\")\n",
    "for x, y in trainldr:\n",
    "    print(f\"x shape: {x.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break\n",
    "\n",
    "net = MLP()\n",
    "print(net)\n",
    "# 你可以調整 epochs 和 lr 來觀察不同的訓練效果\n",
    "train_loss, train_acc, valid_loss, valid_acc = train(\n",
    "    net, trainldr, validldr, epochs=10, lr=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baae834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 39249.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss = 2.301886809333283, test_acc = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Using test_images and test_labels to do the final test\n",
    "test_loss, test_acc = evaluate(net, testldr)\n",
    "print(f\"test_loss = {test_loss}, test_acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0b93b",
   "metadata": {},
   "source": [
    "## 3. Plot loss & accuracy curves(both Training and Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85d6f3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     23\u001b[39m     ax1.set_xlabel(\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m     plt.show()\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mplt_loss_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m plt_acc_all()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mplt_loss_all\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     16\u001b[39m ax1 = fig.add_subplot(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     17\u001b[39m ax1.set_title(\u001b[33m'\u001b[39m\u001b[33mAll loss\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m ax1.plot(\u001b[43mtrain_loss\u001b[49m)\n\u001b[32m     20\u001b[39m ax1.plot(valid_loss)\n\u001b[32m     22\u001b[39m ax1.legend([\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvalid_loss\u001b[39m\u001b[33m'\u001b[39m], loc=\u001b[33m'\u001b[39m\u001b[33mupper left\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_loss' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training and validation loss and accuracy curves\n",
    "def plt_acc_all():\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.set_title('All acc')\n",
    "\n",
    "    ax1.plot(train_acc)\n",
    "    ax1.plot(valid_acc)\n",
    "\n",
    "    ax1.legend(['train_acc', 'valid_acc'], loc='upper left')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "def plt_loss_all():\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.set_title('All loss')\n",
    "\n",
    "    ax1.plot(train_loss)\n",
    "    ax1.plot(valid_loss)\n",
    "\n",
    "    ax1.legend(['train_loss', 'valid_loss'], loc='upper left')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt_loss_all()\n",
    "plt_acc_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
