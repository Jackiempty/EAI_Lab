{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b00152",
   "metadata": {},
   "source": [
    "# Lab 1 Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc389ae1",
   "metadata": {},
   "source": [
    "## 1. Implement the following layers as Python functions (both forward and backward propagation)\n",
    "* Inner-product layer\n",
    "* Activation layer(Sigmoid or Rectified)\n",
    "* Softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee38f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, Iterable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce009b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    def __init__(self, data: np.ndarray) -> None:\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "\n",
    "\n",
    "class Module:\n",
    "    def __call__(self, *args, **kwargs) -> np.ndarray:\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        layers = '\\n'.join([f'  ({k}): {v}' for k, v in self.__dict__.items()])\n",
    "        return f'{self.__class__.__name__}(\\n{layers}\\n)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a8912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features) -> None:\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        init_factor = 0.01\n",
    "        self.W = Parameter(np.random.randn(in_features, out_features) * init_factor)\n",
    "        self.b = Parameter(np.zeros((1, out_features)))\n",
    "\n",
    "        # Cache for backward pass\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 學生實作部分：return output of linear layer\n",
    "        self.x = x\n",
    "        out = x @ self.W.data + self.b.data\n",
    "        return out\n",
    "\n",
    "    def backward(self, dy):\n",
    "        # 學生實作部分：return gradient w.r.t. input and compute gradients for weights and biases\n",
    "        dx = dy @ self.W.data.T\n",
    "        self.W.grad = self.x.T @ dy\n",
    "        self.b.grad = np.sum(dy, axis=0, keepdims=True)\n",
    "        return dx\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}(in_features={self.in_features}, out_features={self.out_features})'\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 學生實作部分：return output of ReLU activation\n",
    "        self.x = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "\n",
    "    def backward(self, dy):\n",
    "        # 學生實作部分：return gradient w.r.t. input\n",
    "        dx = dy * (self.x > 0)\n",
    "        return dx\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'\n",
    "\n",
    "\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 學生實作部分：return output of Sigmoid activation\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        # 學生實作部分：return gradient w.r.t. input\n",
    "        dx = dy * self.y * (1 - self.y)\n",
    "        return dx\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'\n",
    "\n",
    "\n",
    "class Softmax(Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 學生實作部分：return output of Softmax activation\n",
    "        shifted_x = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(shifted_x)\n",
    "        self.y = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return self.y\n",
    "\n",
    "\n",
    "    def backward(self, dy):\n",
    "        # 這邊我們設定同學們會將Softmax和Cross-Entropy Loss一起使用\n",
    "        # 因此backward pass根據講義上的說明使用簡化的版本在Cross-Entropy Loss的部分完成\n",
    "        # 若同學的softmax沒有和Cross-Entropy Loss一起使用，請自行實作完整的backward pass\n",
    "        return dy\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60da9b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    def __init__(self, in_features=784, hidden_features=128, num_classes=10) -> None:\n",
    "        # 學生實作部分：design your Model architecture here\n",
    "        # super().__init__()\n",
    "        self.fc1 = Linear(in_features, hidden_features)\n",
    "        self.relu = ReLU()\n",
    "        self.fc2 = Linear(hidden_features, num_classes)\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 學生實作部分：compute forward pass through your model\n",
    "        out = self.fc1.forward(x)\n",
    "        out = self.relu.forward(out)\n",
    "        out = self.fc2.forward(out)\n",
    "        out = self.softmax.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dy):\n",
    "        # 學生實作部分：compute backward pass through your model\n",
    "        dy = self.fc2.backward(dy)\n",
    "        dy = self.relu.backward(dy)\n",
    "        dy = self.fc1.backward(dy)\n",
    "        return dy\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.fc1.parameters() + self.fc2.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896d12f",
   "metadata": {},
   "source": [
    "## 2. Implement training and testing process\n",
    "* included cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd582fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST:\n",
    "    # root請根據你的檔案位置更改\n",
    "    def __init__(self, root='./data', train=True, transform: Callable = None) -> None:\n",
    "        path = os.path.join(root, 'mnist_train.csv' if train else 'mnist_test.csv')\n",
    "        self.data = np.loadtxt(path, delimiter=',')\n",
    "        self.transform = transform\n",
    "        self.image_size = 28\n",
    "        self.num_classes = 10\n",
    "        self.classes = np.arange(self.num_classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = (self.data[idx, 0] == self.classes).astype(\n",
    "            np.float32\n",
    "        )  # one-hot encoding\n",
    "        image = (\n",
    "            self.data[idx, 1:]\n",
    "            .reshape(self.image_size * self.image_size)\n",
    "            .astype(np.float32)\n",
    "        )\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class Subset:\n",
    "    def __init__(self, dataset, indices: Iterable) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.dataset) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for start_idx in range(0, len(self.dataset), self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, len(self.dataset))\n",
    "            batch_indices = self.indices[start_idx:end_idx]\n",
    "\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "\n",
    "            for idx in batch_indices:\n",
    "                image, label = self.dataset[idx]\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "\n",
    "            yield np.array(batch_images), np.array(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f743cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate train_imgs, train_labels into training and validation\n",
    "# root請根據你的檔案位置更改\n",
    "def load_mnist_data(\n",
    "    root=\"./data\", batch_size=1, split_ratio=0.1, transform=None\n",
    ") -> tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    def _split_dataset(dataset, split_ratio):\n",
    "        # 學生實作部分：split dataset into training and validation sets\n",
    "        # hint: return Subset(dataset, train_indices), Subset(dataset, valid_indices)\n",
    "        # 1. total length and index list\n",
    "        dataset_size = len(dataset)\n",
    "        indices = np.arange(dataset_size)\n",
    "\n",
    "        # 2. shuffle\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        # 3. calculate splitting point\n",
    "        split = int(split_ratio * dataset_size)\n",
    "        train_indices = indices[split:]\n",
    "        valid_indices = indices[:split]\n",
    "        return Subset(dataset, train_indices), Subset(dataset, valid_indices)\n",
    "        \n",
    "        \n",
    "\n",
    "    trainset = MNIST(root=root, train=True, transform=transform)\n",
    "    testset = MNIST(root=root, train=False, transform=transform)\n",
    "    trainset, validset = _split_dataset(trainset, split_ratio=split_ratio)\n",
    "    trainldr = DataLoader(trainset, batch_size=batch_size)\n",
    "    validldr = DataLoader(validset, batch_size=batch_size)\n",
    "    testldr = DataLoader(testset, batch_size=batch_size)\n",
    "    return trainldr, validldr, testldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcfe45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Module):\n",
    "    def __init__(self, epsilon=1e-15) -> None:\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon  # small value to avoid log(0)\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.y_pred = np.clip(y_pred, self.epsilon, 1 - self.epsilon)\n",
    "        self.y_true = y_true\n",
    "        batch_size = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(self.y_pred)) / batch_size\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        batch_size = self.y_true.shape[0]\n",
    "        grad = (self.y_pred - self.y_true) / batch_size\n",
    "        return grad\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, params: Iterable, lr: float = 1e-3) -> None:\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.data -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defbfc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model: Module, trainldr: Iterable, criterion, optimizer) -> tuple[float, float]:\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(trainldr):\n",
    "        # 學生實作部分：complete the training process through one epoch\n",
    "        # ========== 1. forward ==========\n",
    "        out = model.forward(x)  # forward pass\n",
    "\n",
    "        # ========== 2. compute loss ==========\n",
    "        loss = criterion(out, y)\n",
    "        total_loss += float(loss) * len(y)\n",
    "        total += len(y)\n",
    "\n",
    "        # ========== 3. compute accuracy ==========\n",
    "        pred = np.argmax(out, axis=1)\n",
    "        y_idx = np.argmax(y, axis=1)\n",
    "        correct += np.sum(pred == y_idx)\n",
    "\n",
    "        # ========== 4. backward ==========\n",
    "        dy = criterion.backward()     # dL/dy_pred\n",
    "        model.backward(dy)\n",
    "\n",
    "        # ========== 5. update ==========\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Hint:\n",
    "        # 1. forward propagation\n",
    "        # 2. compute loss\n",
    "        # 3. compute accuracy\n",
    "        # 4. backward propagation\n",
    "        # 5. update parameters\n",
    "        \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model: Module, testldr: Iterable) -> tuple[float, float]:\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    criterion = CrossEntropyLoss()  # separate criterion from training\n",
    "    for data, target in tqdm(testldr):\n",
    "        # complete the evaluation process\n",
    "        # ========== 1. forward ==========\n",
    "        out = model.forward(data)\n",
    "\n",
    "        # ========== 2. compute loss ==========\n",
    "        loss = criterion(out, target)\n",
    "        total_loss += float(loss) * len(y)\n",
    "\n",
    "        # ========== 3. compute accuracy ==========\n",
    "        pred = np.argmax(out, axis=1)\n",
    "        target_idx = np.argmax(target, axis=1)\n",
    "        correct += np.sum(pred == target_idx)\n",
    "        total += len(target)\n",
    "        # Hint:\n",
    "        # 1. forward propagation\n",
    "        # 2. compute total loss\n",
    "        # 3. compute correct and total\n",
    "\n",
    "    print(\"total: \", total)\n",
    "    print(\"correct: \", correct)\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train(model: MLP, trainldr: Iterable, validldr: Iterable, epochs=5, lr=1e-3):\n",
    "    criterion = CrossEntropyLoss()\n",
    "    # 這邊提供SGD作為optimizer，同學也可以根據自己人需求更換其他optimizer\n",
    "    optimizer = SGD(model.parameters(), lr=lr)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    valid_loss = []\n",
    "    valid_acc = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss, acc = train_one_epoch(model, trainldr, criterion, optimizer)\n",
    "        train_loss.append(loss)\n",
    "        train_acc.append(acc)\n",
    "        print(f'epoch {epoch:d}: train_loss = {loss}, train_acc = {acc}')\n",
    "\n",
    "        loss, acc = evaluate(model, validldr)\n",
    "        valid_loss.append(loss)\n",
    "        valid_acc.append(acc)\n",
    "        print(f'epoch {epoch:d}: valid_loss = {loss}, valid_acc = {acc}\\n')\n",
    "\n",
    "    return train_loss, train_acc, valid_loss, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72aea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(x):\n",
    "    \"\"\"map pixels information from range(0, 255) to range(0.01, 1)\"\"\"\n",
    "    return np.asarray(x) / 255.0 * 0.99 + 0.01\n",
    "\n",
    "# \"../data\"請根據你的檔案位置更改\n",
    "trainldr, validldr, testldr = load_mnist_data(\n",
    "    \"./data\", batch_size=1, transform=transform\n",
    ")\n",
    "print(f\"train set: {len(trainldr)} images\")\n",
    "print(f\"validation set: {len(validldr)} images\")\n",
    "print(f\"test set: {len(testldr)} images\")\n",
    "for x, y in trainldr:\n",
    "    print(f\"x shape: {x.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break\n",
    "\n",
    "net = MLP()\n",
    "print(net)\n",
    "# 你可以調整 epochs 和 lr 來觀察不同的訓練效果\n",
    "train_loss, train_acc, valid_loss, valid_acc = train(\n",
    "    net, trainldr, validldr, epochs=10, lr=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using test_images and test_labels to do the final test\n",
    "test_loss, test_acc = evaluate(net, testldr)\n",
    "print(f\"test_loss = {test_loss}, test_acc = {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0b93b",
   "metadata": {},
   "source": [
    "## 3. Plot loss & accuracy curves(both Training and Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85d6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss and accuracy curves\n",
    "def plt_acc_all():\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.set_title('All acc')\n",
    "\n",
    "    ax1.plot(train_acc)\n",
    "    ax1.plot(valid_acc)\n",
    "\n",
    "    ax1.legend(['train_acc', 'valid_acc'], loc='upper left')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "def plt_loss_all():\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.set_title('All loss')\n",
    "\n",
    "    ax1.plot(train_loss)\n",
    "    ax1.plot(valid_loss)\n",
    "\n",
    "    ax1.legend(['train_loss', 'valid_loss'], loc='upper left')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt_loss_all()\n",
    "plt_acc_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
